{
"Class" : "edu.stanford.nlp.process.AbstractTokenizer", 
"Methods" : [{ "Name" : "next" ,
"Duas" : 12,
"0" :  "(52,(52,53), this)",
"1" :  "(52,(52,55), this)",
"2" :  "(52,55, this)",
"3" :  "(52,56, this)",
"4" :  "(52,53, this)",
"5" :  "(52,(52,53), this.nextToken)",
"6" :  "(52,(52,55), this.nextToken)",
"7" :  "(52,55, this.nextToken)",
"8" :  "(53,55, this.nextToken)",
"9" :  "(55,(57,58), result)",
"10" :  "(55,(57,60), result)",
"11" :  "(55,60, result)"},{ "Name" : "hasNext" ,
"Duas" : 11,
"0" :  "(68,(68,69), this)",
"1" :  "(68,(68,71), this)",
"2" :  "(68,(71,71), this)",
"3" :  "(68,(71,71), this)",
"4" :  "(68,69, this)",
"5" :  "(68,(68,69), this.nextToken)",
"6" :  "(68,(68,71), this.nextToken)",
"7" :  "(68,(71,71), this.nextToken)",
"8" :  "(68,(71,71), this.nextToken)",
"9" :  "(69,(71,71), this.nextToken)",
"10" :  "(69,(71,71), this.nextToken)"},{ "Name" : "peek" ,
"Duas" : 14,
"0" :  "(91,(91,92), this)",
"1" :  "(91,(91,94), this)",
"2" :  "(91,(94,95), this)",
"3" :  "(91,(94,97), this)",
"4" :  "(91,97, this)",
"5" :  "(91,92, this)",
"6" :  "(91,(91,92), this.nextToken)",
"7" :  "(91,(91,94), this.nextToken)",
"8" :  "(91,(94,95), this.nextToken)",
"9" :  "(91,(94,97), this.nextToken)",
"10" :  "(91,97, this.nextToken)",
"11" :  "(92,(94,95), this.nextToken)",
"12" :  "(92,(94,97), this.nextToken)",
"13" :  "(92,97, this.nextToken)"},{ "Name" : "tokenize" ,
"Duas" : 8,
"0" :  "(110,(111,112), this)",
"1" :  "(110,(111,116), this)",
"2" :  "(110,112, this)",
"3" :  "(110,(116,117), result)",
"4" :  "(110,(116,119), result)",
"5" :  "(110,119, result)",
"6" :  "(110,117, result)",
"7" :  "(110,112, result)"}]
}