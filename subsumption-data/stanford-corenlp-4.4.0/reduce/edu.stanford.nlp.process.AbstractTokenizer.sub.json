{
"Class" : "edu.stanford.nlp.process.AbstractTokenizer", 
"Methods" : [{ "Name" : "next" ,
"Duas" : "12" ,
"Subsumers" : 4,
"0" : [ 8, 5, 4, 0], "S0" : [0, 2, 3, 4, 5, 8 ],
"1" : [ 7, 6, 1], "S1" : [1, 2, 3, 6, 7 ],
"2" : [ 9], "S2" : [2, 3, 9 ],
"3" : [ 11, 10], "S3" : [2, 3, 10, 11 ]
},{ "Name" : "hasNext" ,
"Duas" : "11" ,
"Subsumers" : 4,
"0" : [ 9], "S0" : [0, 2, 4, 5, 9 ],
"1" : [ 10], "S1" : [0, 3, 4, 5, 10 ],
"2" : [ 7], "S2" : [1, 2, 6, 7 ],
"3" : [ 8], "S3" : [1, 3, 6, 8 ]
},{ "Name" : "peek" ,
"Duas" : "14" ,
"Subsumers" : 4,
"0" : [ 11], "S0" : [0, 2, 5, 6, 11 ],
"1" : [ 13, 12], "S1" : [0, 3, 4, 5, 6, 12, 13 ],
"2" : [ 8], "S2" : [1, 2, 7, 8 ],
"3" : [ 10, 9], "S3" : [1, 3, 4, 7, 9, 10 ]
},{ "Name" : "tokenize" ,
"Duas" : "8" ,
"Subsumers" : 3,
"0" : [ 7, 2, 0], "S0" : [0, 1, 2, 5, 7 ],
"1" : [ 6, 3], "S1" : [1, 3, 5, 6 ],
"2" : [ 4], "S2" : [1, 4, 5 ]
}]
}