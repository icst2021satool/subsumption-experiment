{
"Class" : "edu.stanford.nlp.international.arabic.process.ArabicTokenizerTester", 
"Methods" : [{ "Name" : "main" ,
"Duas" : 57,
"0" :  "(32,(32,33), args)",
"1" :  "(32,(32,36), args)",
"2" :  "(32,36, args)",
"3" :  "(32,37, args)",
"4" :  "(32,52, out)",
"5" :  "(32,33, out)",
"6" :  "(32,38, log)",
"7" :  "(32,88, err)",
"8" :  "(32,80, err)",
"9" :  "(32,75, err)",
"10" :  "(32,65, err)",
"11" :  "(40,47, br)",
"12" :  "(40,(47,48), br)",
"13" :  "(40,(47,88), br)",
"14" :  "(41,51, tf)",
"15" :  "(43,58, lexMapper)",
"16" :  "(46,88, lineId)",
"17" :  "(46,47, lineId)",
"18" :  "(47,48, line)",
"19" :  "(48,80, line)",
"20" :  "(48,65, line)",
"21" :  "(51,(64,65), tokenizedLine)",
"22" :  "(51,(64,70), tokenizedLine)",
"23" :  "(51,82, tokenizedLine)",
"24" :  "(51,73, tokenizedLine)",
"25" :  "(51,67, tokenizedLine)",
"26" :  "(55,61, sb)",
"27" :  "(55,59, sb)",
"28" :  "(61,(64,65), mappedToks)",
"29" :  "(61,(64,70), mappedToks)",
"30" :  "(61,(71,72), mappedToks)",
"31" :  "(61,(71,79), mappedToks)",
"32" :  "(61,83, mappedToks)",
"33" :  "(61,72, mappedToks)",
"34" :  "(61,68, mappedToks)",
"35" :  "(70,(79,80), printLines)",
"36" :  "(70,(79,47), printLines)",
"37" :  "(71,(71,72), i)",
"38" :  "(71,(71,79), i)",
"39" :  "(71,72, i)",
"40" :  "(71,73, i)",
"41" :  "(71,71, i)",
"42" :  "(72,(74,75), mappedTok)",
"43" :  "(72,(74,71), mappedTok)",
"44" :  "(72,75, mappedTok)",
"45" :  "(73,(74,75), tokenizedTok)",
"46" :  "(73,(74,71), tokenizedTok)",
"47" :  "(73,75, tokenizedTok)",
"48" :  "(76,(79,80), printLines)",
"49" :  "(76,(79,47), printLines)",
"50" :  "(71,(71,72), i)",
"51" :  "(71,(71,79), i)",
"52" :  "(71,72, i)",
"53" :  "(71,73, i)",
"54" :  "(71,71, i)",
"55" :  "(47,88, lineId)",
"56" :  "(47,47, lineId)"}]
}