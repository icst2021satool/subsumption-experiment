{
"Class" : "edu.stanford.nlp.process.PTBTokenizer$PTBTokenizerFactory", 
"Methods" : [{ "Name" : "<init>" ,
"Duas" : "12" ,
"Subsumers" : 6,
"0" : [ 10, 1], "S0" : [0, 1, 8, 10 ],
"1" : [ 2], "S1" : [0, 2, 8 ],
"2" : [ 9, 3], "S2" : [0, 3, 8, 9 ],
"3" : [ 4], "S3" : [0, 4, 8 ],
"4" : [ 11, 5], "S4" : [0, 5, 8, 11 ],
"5" : [ 7, 6], "S5" : [0, 6, 7, 8 ]
},{ "Name" : "getTokenizer" ,
"Duas" : "17" ,
"Subsumers" : 3,
"0" : [ 12, 3], "S0" : [0, 2, 3, 6, 8, 10, 12, 15 ],
"1" : [ 16, 14, 13, 9, 7, 5, 4], "S1" : [0, 4, 5, 7, 9, 10, 13, 14, 16 ],
"2" : [ 11, 1], "S2" : [1, 2, 6, 8, 11, 15 ]
}]
}