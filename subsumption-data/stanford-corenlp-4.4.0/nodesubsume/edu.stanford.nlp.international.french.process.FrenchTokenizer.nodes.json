{
"Class" : "edu.stanford.nlp.international.french.process.FrenchTokenizer", 
"Methods" : [{ "Name" : "<init>" ,
"Nodes" : 5,
"0" : [ 83,84,85,86,87 ],
"1" : [ 87 ],
"2" : [ 88 ],
"3" : [ 87 ],
"4" : [  ]
},{ "Name" : "getNext" ,
"Nodes" : 20,
"0" : [ 94 ],
"1" : [ 99 ],
"2" : [ 99 ],
"3" : [ 100,101 ],
"4" : [ 101,102 ],
"5" : [ 105 ],
"6" : [ 113 ],
"7" : [ 120 ],
"8" : [ 113 ],
"9" : [ 114,115 ],
"10" : [ 115 ],
"11" : [ 116 ],
"12" : [ 105 ],
"13" : [ 106,107 ],
"14" : [ 107 ],
"15" : [ 108 ],
"16" : [ 102 ],
"17" : [ 99,100 ],
"18" : [ 99 ],
"19" : [  ]
},{ "Name" : "processCompound" ,
"Nodes" : 5,
"0" : [ 146,147,148 ],
"1" : [ 148 ],
"2" : [ 155 ],
"3" : [ 148,149,150,151,152,153 ],
"4" : [  ]
},{ "Name" : "processContraction" ,
"Nodes" : 14,
"0" : [ 168,169,172,174,175 ],
"1" : [ 175 ],
"2" : [ 175 ],
"3" : [ 175 ],
"4" : [ 175 ],
"5" : [ 175 ],
"6" : [ 175 ],
"7" : [ 175 ],
"8" : [ 192,193,189,190,191 ],
"9" : [ 198,199,200,201 ],
"10" : [ 183,184,185,186,187 ],
"11" : [ 177,178,179,180,181 ],
"12" : [ 195 ],
"13" : [  ]
},{ "Name" : "main" ,
"Nodes" : 21,
"0" : [ 355,356 ],
"1" : [ 362 ],
"2" : [ 363 ],
"3" : [ 363,364,367 ],
"4" : [ 367 ],
"5" : [ 368,371,372,375,376,377,379,380,367 ],
"6" : [ 381 ],
"7" : [ 400,401,397,398,399 ],
"8" : [ 384,382,383 ],
"9" : [ 389 ],
"10" : [ 390 ],
"11" : [ 390 ],
"12" : [ 390,391,392 ],
"13" : [ 394 ],
"14" : [ 390 ],
"15" : [ 389 ],
"16" : [ 385,386,387 ],
"17" : [ 367 ],
"18" : [ 363 ],
"19" : [ 357,358 ],
"20" : [  ]
}]
}