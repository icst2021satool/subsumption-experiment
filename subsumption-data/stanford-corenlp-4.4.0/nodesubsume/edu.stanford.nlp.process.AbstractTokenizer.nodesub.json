{
"Class" : "edu.stanford.nlp.process.AbstractTokenizer", 
"Methods" : [{ "Name" : "next" ,
"Nodes" : 6,
"0" : [ ],
"4" : [ 0, 4, 5],
"1" : [ 2, 3],
"2" : [ 2, 3, 10, 11],
"3" : [ 2, 3, 9],
"5" : [ 2, 3],
"CoveredDUAsByNodes" : 8,
"Duas" : 12
},{ "Name" : "hasNext" ,
"Nodes" : 7,
"0" : [ ],
"5" : [ 0, 4, 5],
"1" : [ ],
"2" : [ 3],
"4" : [ 2],
"3" : [ ],
"6" : [ ],
"CoveredDUAsByNodes" : 5,
"Duas" : 11
},{ "Name" : "peek" ,
"Nodes" : 6,
"0" : [ ],
"4" : [ 0, 5, 6],
"1" : [ ],
"2" : [ 3, 4],
"3" : [ 2],
"5" : [ ],
"CoveredDUAsByNodes" : 6,
"Duas" : 14
},{ "Name" : "tokenize" ,
"Nodes" : 7,
"0" : [ ],
"1" : [ ],
"2" : [ 1],
"4" : [ 1, 3, 6],
"3" : [ 1, 5],
"6" : [ 1, 5],
"5" : [ 0, 2, 7],
"CoveredDUAsByNodes" : 7,
"Duas" : 8
}]
}