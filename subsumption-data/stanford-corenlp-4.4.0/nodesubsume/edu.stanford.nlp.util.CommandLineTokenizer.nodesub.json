{
"Class" : "edu.stanford.nlp.util.CommandLineTokenizer", 
"Methods" : [{ "Name" : "appendToBuffer" ,
"Nodes" : 4,
"0" : [ ],
"2" : [ 0, 1, 3, 4],
"1" : [ ],
"3" : [ ],
"CoveredDUAsByNodes" : 4,
"Duas" : 5
},{ "Name" : "tokenize" ,
"Nodes" : 23,
"0" : [ ],
"2" : [ 0, 2],
"3" : [ 0, 2],
"4" : [ 0, 2, 22, 11, 15],
"1" : [ 9, 10],
"22" : [ 9, 10],
"5" : [ 0, 2, 3, 29, 14, 31],
"6" : [ 0, 2, 3, 42, 29, 14, 31],
"7" : [ 0, 2, 3, 42, 44, 29, 14, 31],
"8" : [ 0, 2, 3, 42, 44, 29, 14, 31],
"9" : [ 0, 2, 3, 23, 42, 44, 29, 14, 46, 31, 47],
"11" : [ 0, 2, 3, 24, 42, 12, 44, 29, 45, 14, 31],
"12" : [ 0, 48, 2, 3, 25, 42, 44, 29, 14, 31],
"13" : [ 0, 2, 3, 42, 43, 29, 14, 31],
"15" : [ 0, 16, 2, 3, 42, 43, 29, 14, 31],
"17" : [ 0, 16, 2, 3, 4, 42, 43, 29, 14, 31],
"14" : [ 0, 2, 3, 26, 42, 43, 29, 14, 31],
"16" : [ 0, 16, 2, 3, 6, 42, 27, 43, 29, 14, 31],
"18" : [ 0, 2, 3, 41, 28, 13, 29, 14, 31],
"19" : [ 0, 2, 3, 41, 28, 13, 29, 14, 31],
"21" : [ 0, 2, 3, 41, 28, 13, 29, 14, 31],
"20" : [ 0, 2, 3, 41, 28, 13, 29, 14, 31],
"10" : [ 0, 2, 3, 29, 14, 31],
"CoveredDUAsByNodes" : 30,
"Duas" : 66
}]
}