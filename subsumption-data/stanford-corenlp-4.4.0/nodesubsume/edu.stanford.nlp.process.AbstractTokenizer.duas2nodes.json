{
"Class" : "edu.stanford.nlp.process.AbstractTokenizer", 
"Methods" : [{ "Name" : "next" ,
"Nodes" : 6,
"Duas" : 12,
"0" :  [ 0,4 ],
"1" :  [ 0,1 ],
"2" :  [ 0,1 ],
"3" :  [ 0,1 ],
"4" :  [ 0,4 ],
"5" :  [ 0,4 ],
"6" :  [ 0,1 ],
"7" :  [ 0,1 ],
"8" :  [ 4,1 ],
"9" :  [ 1,3 ],
"10" :  [ 1,2 ],
"11" :  [ 1,2 ]},{ "Name" : "hasNext" ,
"Nodes" : 7,
"Duas" : 11,
"0" :  [ 0,5 ],
"1" :  [ 0,1 ],
"2" :  [ 0,1,4 ],
"3" :  [ 0,1,2 ],
"4" :  [ 0,5 ],
"5" :  [ 0,5 ],
"6" :  [ 0,1 ],
"7" :  [ 0,1,4 ],
"8" :  [ 0,1,2 ],
"9" :  [ 5,1,4 ],
"10" :  [ 5,1,2 ]},{ "Name" : "peek" ,
"Nodes" : 6,
"Duas" : 14,
"0" :  [ 0,4 ],
"1" :  [ 0,1 ],
"2" :  [ 0,1,3 ],
"3" :  [ 0,1,2 ],
"4" :  [ 0,2 ],
"5" :  [ 0,4 ],
"6" :  [ 0,4 ],
"7" :  [ 0,1 ],
"8" :  [ 0,1,3 ],
"9" :  [ 0,1,2 ],
"10" :  [ 0,2 ],
"11" :  [ 4,1,3 ],
"12" :  [ 4,1,2 ],
"13" :  [ 4,2 ]},{ "Name" : "tokenize" ,
"Nodes" : 7,
"Duas" : 8,
"0" :  [ 0,1,5 ],
"1" :  [ 0,1,2 ],
"2" :  [ 0,5 ],
"3" :  [ 0,2,4 ],
"4" :  [ 0,2,3 ],
"5" :  [ 0,3 ],
"6" :  [ 0,4 ],
"7" :  [ 0,5 ]}]
}